pretrained_ckpt: null
use_pretrained_norm_stats: False

# strategy
model_weights_to_bf16: False
enable_bf16_training: False
use_torch_compile: False  # Disabled to avoid checkpoint saving issues
find_unused_parameters: False

# dataloader
batch_size: 4
num_workers: 8
pin_memory: True
persistent_workers: True

# training period
max_epochs: 10
max_steps: null
grad_accumulation_steps: 1

# optimizer
use_8bit_optimizer: False
learning_rate: 2.5e-5
weight_decay: 0.01
betas: [0.9, 0.95]

# LR scheduler
lr_scheduler_type: "cosine"
warmup_steps: 1_000

max_grad_norm: 1.0

# EMA
use_ema: False
ema:
  update_after_step: 0
  power: 0.67

# sync batch normalization
use_sync_bn: False

tokenizer:
  _target_: galaxea_fm.models.open_pi.paligemma.fast_tokenizer.FASTTokenizer
  max_len: 512
  fast_tokenizer_path: "physical-intelligence/fast"
  paligemma_tokenizer_path: /TO/Your/Path/openpi/big_vision/paligemma_tokenizer.model
  pad_token_id: ${model.model_arch.pad_token_id}
  image_token_index: ${model.model_arch.image_token_index}
  num_tokens_per_image: ${model.model_arch.vision.num_image_tokens}
  num_input_images: ${model.model_arch.num_input_images}

model_arch:
  _target_: galaxea_fm.models.open_pi.pi0_fast_policy.Pi0FastPolicy

  pretrained_model_path: /TO/Your/Path/openpi/pytorch/pi0fast-base
  vla_training_strategy: "vla-full-train"
  backbone_lr_multiplier: 1.0
  image_token_index: 257151
  pad_token_id: 0
  vocab_size: 257152

  cond_steps: ${data.dataset.obs_size}
  horizon_steps: ${data.dataset.action_size}
  max_token_len: 512
  num_input_images: ${eval:'${model.model_arch.cond_steps} * ${data.processor.num_output_cameras}'}
  input_image_size:
    - ${model.model_arch.vision.image_size}
    - ${model.model_arch.vision.image_size}
  final_action_clip_value: null

  action_dim: ${data.processor.action_output_dim}

  vision:
    hidden_size: 1152
    intermediate_size: 4304
    num_hidden_layers: 27
    num_attention_heads: 16
    num_channels: 3
    image_size: 224
    patch_size: 14
    hidden_act: gelu_pytorch_tanh
    layer_norm_eps: 0.000001
    attention_dropout: 0.0
    model_type: siglip_vision_model
    projection_dim: 2048
    projector_hidden_act: gelu_fast
    torch_dtype: float32
    vision_use_head: false
    vocab_size: ${model.model_arch.vocab_size}
    num_image_tokens: 256

  vision_projector:
    vision_config:
      hidden_size: 1152
      projection_dim: 2048

  # Single LLM config (Gemma 2B) - no action expert
  llm:
    hidden_size: 2048
    intermediate_size: 16384
    num_hidden_layers: 18
    num_attention_heads: 8
    num_key_value_heads: 1
    head_dim: 256
    max_position_embeddings: 8192
    rms_norm_eps: 0.000001
    rope_theta: 10000.0
    attention_bias: False
    attention_dropout: 0.0
    bos_token_id: 2
    eos_token_id: 1
    hidden_act: gelu_pytorch_tanh
    hidden_activation: gelu_pytorch_tanh
    initializer_range: 0.02
    model_type: gemma
    torch_dtype: float32
    pad_token_id: ${model.model_arch.pad_token_id}
    vocab_size: ${model.model_arch.vocab_size}
    use_cache: True
    use_final_norm: True
    cache: True
    use_adarms: False

model_meta:
  norm_default_mode: q01/q99
  input_image_size:
    - ${model.model_arch.vision.image_size}
    - ${model.model_arch.vision.image_size}
